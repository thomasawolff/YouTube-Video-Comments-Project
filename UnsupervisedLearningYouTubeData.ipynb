{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM1POpCOBwSKZrXVkgTPiY7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomasawolff/YouTube-Video-Comments-Project/blob/main/UnsupervisedLearningYouTubeData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "3bahca9hFMuH",
        "outputId": "99a80ab7-f69f-4eb5-b5d5-03b45ef129be"
      },
      "source": [
        "\r\n",
        "  \r\n",
        "import re\r\n",
        "import os\r\n",
        "import nltk\r\n",
        "import math\r\n",
        "import json\r\n",
        "import nltk.corpus\r\n",
        "import operator\r\n",
        "import textwrap\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "from PIL import Image\r\n",
        "from wordcloud import WordCloud\r\n",
        "from scipy.stats import pearsonr\r\n",
        "from scipy.stats import kurtosis\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "from nltk import ne_chunk\r\n",
        "from textblob import TextBlob\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "import scipy.cluster.hierarchy as sch\r\n",
        "from multiprocessing import Pool\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.neighbors import KernelDensity\r\n",
        "from scipy.stats import norm\r\n",
        "nltk.download(\"stopwords\")\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import wordnet\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "from collections import Counter\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from sklearn.cluster import AgglomerativeClustering\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "\r\n",
        "print(os.chdir('/content/drive/MyDrive/Research_Methods/Course_Project/YouTube_Data/Unicode_Files'))\r\n",
        "#print(os.listdir())\r\n",
        "url = 'youTubeVideosUTF.csv'\r\n",
        "\r\n",
        "\r\n",
        "categories = ['Film & Animation', 'Autos & Vehicles', 'Music', 'Pets & Animals', 'Sports'\r\n",
        "          , 'Short Movies', 'Travel & Events', 'Gaming', 'Videoblogging', 'People & Blogs'\r\n",
        "          , 'Comedy', 'Entertainment', 'News & Politics', 'Howto & Style', 'Education'\r\n",
        "          , 'Science & Technology', 'Nonprofits & Activism', 'Movies', 'Anime/Animation'\r\n",
        "          , 'Action/Adventure', 'Classics', 'Comedy', 'Documentary', 'Drama', 'Family'\r\n",
        "          , 'Foreign', 'Horror', 'Sci-Fi/Fantasy', 'Thriller', 'Shorts', 'Shows', 'Trailers']\r\n",
        "\r\n",
        "print('')\r\n",
        "print('')\r\n",
        "##print(categories)\r\n",
        "##print('')\r\n",
        "##print('')\r\n",
        "\r\n",
        "while True:\r\n",
        "    exit_ = input('Type Done to leave or press enter ')\r\n",
        "    if exit_.lower() == 'done': sys.exit(0)\r\n",
        "    clusters = input('Do you know how many clusters you want? Yes/No: ')\r\n",
        "    if clusters.lower() == 'yes':\r\n",
        "        try:\r\n",
        "            try:\r\n",
        "                go = textAnalytics(url,\r\n",
        "                                   numClusters = int(input('How many clusters do you want?: ')),\r\n",
        "                                   channel = input('Which channel do you want to analyze?: '),\r\n",
        "                                   #category = input('Which category do you want to analyze?: '),\r\n",
        "                                   dataFeature1 = 'videoID', # First of Four columns in dataset\r\n",
        "                                   #dataFeature2 = 'categoryID',\r\n",
        "                                   #dataFeature3 = 'views',\r\n",
        "                                   dataFeature4 = 'commentText',\r\n",
        "                                   sentiment = input('Which sentiment do you want? (1.0 for positive,0.0 for nuetral,-1.0 for negative): '))\r\n",
        "\r\n",
        "                #go.kMeansElbow()\r\n",
        "                go.kMeansVisualizer()\r\n",
        "                go.triGramConverter()\r\n",
        "                go.tagsMaker()\r\n",
        "                go.wordCloudVisualizer()\r\n",
        "                go.plot_similarity()\r\n",
        "            except ValueError:\r\n",
        "                print('You may have entered bad data')\r\n",
        "                pass\r\n",
        "        except TypeError:\r\n",
        "            print('You Entered an invalid cluster number, try: '+str(int(go.cluster)-1))\r\n",
        "            pass\r\n",
        "    else:\r\n",
        "         try:\r\n",
        "            try:\r\n",
        "                 go = textAnalytics(url,\r\n",
        "                                   #category = input('Which category do you want to analyze?: '),\r\n",
        "                                   channel = input('Which channel do you want to analyze?: '),\r\n",
        "                                   dataFeature1 = 'videoID', # First of Four columns in dataset\r\n",
        "                                   #dataFeature2 = 'categoryID',\r\n",
        "                                   dataFeature3 = 'views',\r\n",
        "                                   dataFeature4 = 'commentText',\r\n",
        "                                   sentiment = input('Which sentiment do you want? (1.0 for positive,0.0 for nuetral,-1.0 for negative): '))\r\n",
        "\r\n",
        "                 go.kMeansElbow()\r\n",
        "            except ValueError:\r\n",
        "                print('You may have entered bad data')\r\n",
        "                pass\r\n",
        "         except TypeError:\r\n",
        "            print('You Entered an invalid cluster number, try: '+str(int(go.cluster)-1))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def jSonYield():\r\n",
        "    ids = []\r\n",
        "    titleList = []\r\n",
        "    data = json.load(open('US_category_id.json'))\r\n",
        "    pos0 = data.keys()\r\n",
        "    pos1 = data.get('items')\r\n",
        "    try:\r\n",
        "        for i in range(0,len(pos1)):\r\n",
        "            id_ = pos1[i]['id']\r\n",
        "            title = pos1[i]['snippet']['title']\r\n",
        "            ids.append(id_)\r\n",
        "            titleList.append(title)\r\n",
        "            cats = dict(zip(ids,titleList))\r\n",
        "    except IndexError: pass\r\n",
        "    return cats\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class textAnalytics(object):\r\n",
        "\r\n",
        "    def __init__(self,file1,\r\n",
        "                 numClusters=2,\r\n",
        "                 category=None,\r\n",
        "                 channel=None,\r\n",
        "                 dataFeature1=None,\r\n",
        "                 dataFeature2=None,\r\n",
        "                 dataFeature3=None,\r\n",
        "                 dataFeature4=None,\r\n",
        "                 sentiment = None\r\n",
        "                 ):\r\n",
        "\r\n",
        "        dict_ = jSonYield()\r\n",
        "        self.limit = 100\r\n",
        "        self.stringsList = []\r\n",
        "        self.sentiment = sentiment\r\n",
        "        self.number_clusters = numClusters\r\n",
        "        self.dataFeature1 = dataFeature1\r\n",
        "        self.dataFeature2 = dataFeature2\r\n",
        "        self.dataFeature3 = dataFeature3\r\n",
        "        self.dataFeature4 = dataFeature4\r\n",
        "        categoryPick = pd.DataFrame(dict_.items(),columns=[self.dataFeature2,'category'])\r\n",
        "        data_df = pd.read_csv(file1,low_memory=False)\r\n",
        "        self.token_pattern = '(?u)\\\\b\\\\w+\\\\b'\r\n",
        "        categoryPick[self.dataFeature2] = categoryPick[self.dataFeature2].astype(int)\r\n",
        "        review_df_All = data_df[[self.dataFeature1,self.dataFeature2,self.dataFeature4]]\r\n",
        "        review_df_All = pd.merge(categoryPick, review_df_All, on = self.dataFeature2)\r\n",
        "        videoTitles = pd.read_csv('YouTubeVideoTitles.csv')\r\n",
        "        review_df_All = pd.merge(videoTitles, review_df_All, on = dataFeature1)\r\n",
        "        review_df_All = review_df_All.loc[review_df_All['channel'] == channel]\r\n",
        "        self.stopWords = stopwords.words('english')\r\n",
        "        try:\r\n",
        "            print('There are ',len(review_df_All),' comments on this topic')\r\n",
        "            self.dataComm = review_df_All.sample(10000).drop_duplicates()\r\n",
        "        except ValueError:\r\n",
        "            print('There are ',len(review_df_All),' comments on this topic')\r\n",
        "            self.dataComm = review_df_All.drop_duplicates()\r\n",
        "                \r\n",
        "\r\n",
        "    def sentimentAnalysis(self):\r\n",
        "        pol = []\r\n",
        "        sub = []\r\n",
        "        for i in self.dataComm.commentText.values:\r\n",
        "            try:\r\n",
        "                analysis = TextBlob(i)\r\n",
        "                pol.append(round(analysis.sentiment.polarity,2))\r\n",
        "            except:\r\n",
        "                pol.append(0)\r\n",
        "\r\n",
        "        for i in self.dataComm.commentText.values:\r\n",
        "            try:\r\n",
        "                analysis = TextBlob(i)\r\n",
        "                sub.append(round(analysis.sentiment.subjectivity,2))\r\n",
        "            except:\r\n",
        "                sub.append(0)\r\n",
        "        self.dataComm['polarity']=pol\r\n",
        "        self.dataComm['subjectivity']=sub\r\n",
        "        self.dataComm.loc[self.dataComm['polarity'] < 0, 'sentimentBucket'] = -1\r\n",
        "        self.dataComm.loc[self.dataComm['polarity'] == 0, 'sentimentBucket'] = 0\r\n",
        "        self.dataComm.loc[self.dataComm['polarity'] > 0, 'sentimentBucket'] = 1\r\n",
        "        self.dataComm = self.dataComm.loc[self.dataComm['sentimentBucket'].astype(float) == float(self.sentiment)]\r\n",
        "        #dataComm .to_csv('youTubeVideosSentimentAnalysisSample10000.csv',sep=',',encoding='utf-8')\r\n",
        "        #print(dataComm )\r\n",
        "        ##                    videoID       categoryID  views  ...    replies  polarity   subjectivity\r\n",
        "        ##          251449  LLGENw4C1jk          17   1002386  ...      0.0      0.50          0.50\r\n",
        "        ##          39834   3VVnY86ulA8          22    802134  ...      0.0      0.00          0.10\r\n",
        "        ##          203460  iA86imHKCMw          17   3005399  ...      0.0     -0.08          0.69\r\n",
        "        ##          345225  RRkdV_xmYOI          23    367544  ...      0.0      0.13          0.76\r\n",
        "        ##          402953  vQ3XgMKAgxc          10  51204658  ...      0.0      0.25          0.50\r\n",
        "        \r\n",
        "\r\n",
        "    def distPlotter(self):\r\n",
        "        self.sentimentAnalysis()\r\n",
        "        name = str('commentCount')\r\n",
        "        field = self.dataComm[name]\r\n",
        "        print(round(field.drop_duplicates().describe(include='all')),2)\r\n",
        "        print('Kurtosis:',round(kurtosis(field),2))\r\n",
        "        print('Pearson R Correlation Views/Comments:',pearsonr(self.dataComm ['polarity'],self.dataComm ['subjectivity']))\r\n",
        "        #print('Pearson R Correlation Views/Likes:',pearsonr(self.dataComm ['views'],self.dataComm ['likes']))\r\n",
        "        #print('Pearson R Correlation Views/Dislikes:',pearsonr(self.dataComm ['views'],self.dataComm ['dislikes']))\r\n",
        "        plt.grid(axis='y', alpha=0.50)\r\n",
        "        plt.title('Histogram of '+name1)\r\n",
        "        plt.xlabel(name2)\r\n",
        "        plt.ylabel('Subjectivity')\r\n",
        "        plt.hist(field,bins=70)\r\n",
        "        plt.ticklabel_format(style='plain')\r\n",
        "        #sns.distplot(self.dataComm ['views'],hist=True,fit=norm,kde=False,norm_hist=False)\r\n",
        "        #x,y = sns.kdeplot(self.dataComm ['views']).get_lines()[0].get_data()\r\n",
        "        plt.show()\r\n",
        "\r\n",
        "        \r\n",
        "    def dataModify(self):\r\n",
        "        self.sentimentAnalysis()\r\n",
        "        self.dataComm  = self.dataComm[[self.dataFeature1,self.dataFeature2,self.dataFeature4,'polarity','subjectivity','sentimentBucket']]\r\n",
        "        self.X = self.dataComm.iloc[:,[self.dataComm.columns.get_loc('polarity'),self.dataComm.columns.get_loc('subjectivity')]].values\r\n",
        "\r\n",
        "      \r\n",
        "    def kMeansElbow(self):\r\n",
        "        self.dataModify()\r\n",
        "        # using the elbow method to find optimal number of clusters\r\n",
        "        wcss = []\r\n",
        "        for i in range(1, 11):\r\n",
        "           kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter=300,n_init=10,random_state=0)\r\n",
        "           kmeans.fit(self.X)\r\n",
        "           wcss.append(kmeans.inertia_)\r\n",
        "        plt.plot(range(1, 11),wcss)\r\n",
        "        plt.title('The Elbow Method')\r\n",
        "        plt.xlabel('Number of Data Clusters')\r\n",
        "        plt.ylabel('WCSS')\r\n",
        "        plt.show()\r\n",
        "\r\n",
        "      \r\n",
        "    def kMeansClustering(self):\r\n",
        "        self.dataModify()\r\n",
        "        self.kmeans = KMeans(self.number_clusters, init = 'k-means++',max_iter=300,n_init=10)\r\n",
        "        self.y_kmeans = self.kmeans.fit_predict(self.X)\r\n",
        "        self.dataComm['clusters'] = self.y_kmeans\r\n",
        "       \r\n",
        "\r\n",
        "    def kMeansVisualizer(self):\r\n",
        "        self.kMeansClustering()\r\n",
        "        # visualizing the clusters using K-means\r\n",
        "        for i in range(0,self.number_clusters):\r\n",
        "           plt.scatter(self.X[self.y_kmeans == i, 0], self.X[self.y_kmeans == i, 1], s = 100)\r\n",
        "           #plt.scatter(self.kmeans.cluster_centers_[:,0],self.kmeans.cluster_centers_[:,1],s=50,c='yellow',label='Centroids')\r\n",
        "        plt.title('Clusters of Sentiment vs Subjectivity: K-Means Method')\r\n",
        "        plt.xlabel('Sentiment Value')\r\n",
        "        plt.ylabel('Subjectivity Value')\r\n",
        "        plt.legend(set(self.y_kmeans))\r\n",
        "        plt.show()\r\n",
        "        cluster = int(input('Which cluster do you want to analyze?: '))\r\n",
        "        self.dataComm = self.dataComm.loc[self.dataComm['clusters'] == cluster]\r\n",
        "        self.dataComm.to_csv('dataComm.csv')\r\n",
        "\r\n",
        "\r\n",
        "    def bowConverter(self):\r\n",
        "        dataComm = pd.read_csv('dataComm.csv')\r\n",
        "        bow_converter = CountVectorizer(token_pattern=self.token_pattern)\r\n",
        "        x = bow_converter.fit_transform(dataComm[self.dataFeature4])\r\n",
        "        self.words = bow_converter.get_feature_names()\r\n",
        "        #print(len(words)) ## 29221\r\n",
        "\r\n",
        "        \r\n",
        "    def biGramConverter(self):\r\n",
        "        dataComm = pd.read_csv('dataComm.csv')\r\n",
        "        bigram_converter = CountVectorizer(ngram_range=(2,2), token_pattern=self.token_pattern)\r\n",
        "        x2 = bigram_converter.fit_transform(dataComm[self.dataFeature4])\r\n",
        "        self.bigrams = bigram_converter.get_feature_names()\r\n",
        "        #print(len(bigrams)) ## 368937\r\n",
        "        #print(bigrams[-10:])\r\n",
        "        ##        ['zuzu was', 'zuzus room', 'zweigel wine'\r\n",
        "        ##       , 'zwiebel kräuter', 'zy world', 'zzed in'\r\n",
        "        ##       , 'éclairs napoleons', 'école lenôtre', 'ém all', 'òc châm']\r\n",
        "\r\n",
        "\r\n",
        "    def triGramConverter(self):\r\n",
        "        dataComm = pd.read_csv('dataComm.csv')\r\n",
        "        trigram_converter = CountVectorizer(ngram_range=(3,3), token_pattern=self.token_pattern)\r\n",
        "        x3 = trigram_converter.fit_transform(dataComm[self.dataFeature4])\r\n",
        "        self.trigrams = trigram_converter.get_feature_names()\r\n",
        "        print(len(self.trigrams)) # 881609\r\n",
        "        print(self.trigrams[400:450])\r\n",
        "        ##        ['0 0 eye', '0 20 less', '0 39 oz', '0 39 pizza', '0 5 i'\r\n",
        "        ##         , '0 50 to', '0 6 can', '0 75 oysters', '0 75 that', '0 75 to']\r\n",
        "\r\n",
        "\r\n",
        "    def gramPlotter(self):\r\n",
        "        self.bowConverter()\r\n",
        "        self.biGramConverter()\r\n",
        "        self.triGramConverter()\r\n",
        "        \r\n",
        "        sns.set_style(\"darkgrid\")\r\n",
        "        counts = [len(self.words), len(self.bigrams), len(self.trigrams)]\r\n",
        "        plt.plot(counts, color='cornflowerblue')\r\n",
        "        plt.plot(counts, 'bo')\r\n",
        "        plt.margins(0.1)\r\n",
        "        plt.xticks(range(3), ['unigram', 'bigram', 'trigram'])\r\n",
        "        plt.tick_params(labelsize=14)\r\n",
        "        plt.title('Number of ngrams in the first 10,000 reviews of the dataset', {'fontsize':16})\r\n",
        "        plt.show()\r\n",
        "\r\n",
        "\r\n",
        "    def wordCount(self):\r\n",
        "        dataComm = pd.read_csv('dataComm.csv')\r\n",
        "        for line in dataComm[self.dataFeature4]:\r\n",
        "            wordsTokens = word_tokenize(line)\r\n",
        "            self.stringsList.append(Counter(wordsTokens))\r\n",
        "        ##print(self.stringsList)\r\n",
        "        ##  Counter({'.': 11, 'the': 9, 'and': 8, 'was': 8, 'It': 5, 'I': 5, 'it': 4, 'their': 4\r\n",
        "\r\n",
        "\r\n",
        "    def stringCleaning(self):\r\n",
        "        self.wordCount()\r\n",
        "        lengthList = []\r\n",
        "        punctuationList = ['-?','!',',',':',';','()',\"''\",'.',\"``\",'|','^','..','...','--','=']\r\n",
        "        for i in range(0,self.limit):\r\n",
        "            try:\r\n",
        "                for words in self.stringsList[i]:\r\n",
        "                    if len(words)>0:\r\n",
        "                        lengthList.append(words)\r\n",
        "            except IndexError: pass\r\n",
        "        post_punctuation = [word for word in lengthList if word not in punctuationList]\r\n",
        "        noStopWords = [word for word in post_punctuation if word not in self.stopWords]\r\n",
        "        self.postPunctCount = Counter(noStopWords)\r\n",
        "        # print(self.postPunctCount)\r\n",
        "        ##        Counter({'I': 9, \"n't\": 6, 'The': 5, 'go': 5, 'good': 5, \"'s\": 5,\r\n",
        "        ##                 'My': 4, 'It': 4, 'place': 4, 'menu': 4, ')': 4, 'outside': 3,\r\n",
        "        ##                 'food': 3, 'like': 3, \"'ve\": 3, 'amazing': 3, 'delicious': 3,\r\n",
        "        ##                 'came': 3, 'wait': 3, 'back': 3, 'They': 3, 'evening': 3, 'try': 3,\r\n",
        "        ##                 'one': 3, '(': 3, 'awesome': 3,'much': 3, 'took': 2, 'made': 2,\r\n",
        "        ##                 'sitting': 2, 'Our': 2, 'arrived': 2, 'quickly': 2, 'looked': 2, ....\r\n",
        "\r\n",
        "\r\n",
        "    def tagsMaker(self):\r\n",
        "        # If you want to run this code, install Ghostscript first\r\n",
        "        self.stringCleaning()\r\n",
        "        tags = nltk.pos_tag(self.postPunctCount)\r\n",
        "        grams = ne_chunk(tags)\r\n",
        "        grammers = r\"NP: {<DT>?<JJ>*<NN>}\"\r\n",
        "        chunk_parser = nltk.RegexpParser(grammers)\r\n",
        "        chunk_result = chunk_parser.parse(grams)\r\n",
        "        print(chunk_result)\r\n",
        "        ##        (ORGANIZATION General/NNP Manager/NNP Scott/NNP Petello/NNP)\r\n",
        "        ##          (NP egg/NN)\r\n",
        "        ##          Not/RB\r\n",
        "        ##          (NP detail/JJ assure/NN)\r\n",
        "        ##          albeit/IN\r\n",
        "        ##          (NP rare/JJ speak/JJ treat/NN)\r\n",
        "        ##          (NP guy/NN)\r\n",
        "        ##          (NP respect/NN)\r\n",
        "        ##          (NP state/NN)\r\n",
        "        ##          'd/MD\r\n",
        "        ##          surprised/VBN\r\n",
        "        ##          walk/VB\r\n",
        "        ##          totally/RB\r\n",
        "        ##          satisfied/JJ\r\n",
        "        ##          Like/IN\r\n",
        "        ##          always/RB\r\n",
        "        ##          say/VBP\r\n",
        "        ##          (PERSON Mistakes/NNP)\r\n",
        "\r\n",
        "\r\n",
        "    def wordCloudVisualizer(self):\r\n",
        "        dataComm = pd.read_csv('dataComm.csv')\r\n",
        "        wordcloud = WordCloud(\r\n",
        "            background_color='white',\r\n",
        "            stopwords= [\"dtype\",'commentText','object','video','Name']+stopwords.words('english'),\r\n",
        "            max_words=200,\r\n",
        "            max_font_size=40, \r\n",
        "            scale=3\r\n",
        "            ).generate(str(dataComm['commentText']))\r\n",
        "        fig = plt.figure(1, figsize=(6,6))\r\n",
        "        plt.title('Word cloud of chosen cluster')\r\n",
        "        plt.axis('off')\r\n",
        "        plt.imshow(wordcloud)\r\n",
        "        plt.show()\r\n",
        "\r\n",
        "\r\n",
        "    # this project uses GPU processing in tensorflow for embedding the comments\r\n",
        "    def embed_useT(self,module):\r\n",
        "        with tf.Graph().as_default():\r\n",
        "            gpus = tf.config.experimental.list_physical_devices('GPU')\r\n",
        "            if gpus:\r\n",
        "                try:\r\n",
        "                    tf.config.experimental.set_virtual_device_configuration(\r\n",
        "                    gpus[1],\r\n",
        "                    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000),\r\n",
        "                      tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\r\n",
        "                    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n",
        "                    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\r\n",
        "                except RuntimeError as e:\r\n",
        "                    print(e)\r\n",
        "                \r\n",
        "            else: tf.config.experimental.list_physical_devices(device_type='CPU')\r\n",
        "        \r\n",
        "            sentences = tf.placeholder(tf.string)\r\n",
        "            embed = hub.Module(module)\r\n",
        "            embeddings = embed(sentences)\r\n",
        "            session = tf.train.MonitoredSession()\r\n",
        "        return lambda x: session.run(embeddings, {sentences: x})\r\n",
        "\r\n",
        "\r\n",
        "    # visualizes the heatmap for correlation from inner product between the sentence vectors.\r\n",
        "    def plot_similarity(self):\r\n",
        "        dataComm = pd.read_csv('dataComm.csv')\r\n",
        "        comments = dataComm['commentText'].sample(10)\r\n",
        "        self.random = '\"'+comments+'\"'\r\n",
        "        embed_fn = self.embed_useT(r'C:\\Users\\moose_f8sa3n2\\Google Drive\\Research Methods\\Course Project\\YouTube Data\\Unicode Files')\r\n",
        "        encoding_matrix = embed_fn(self.random)\r\n",
        "        products = np.inner(encoding_matrix, encoding_matrix)\r\n",
        "        mask = np.zeros_like(products, dtype=np.bool)\r\n",
        "        mask[np.triu_indices_from(mask)] = True\r\n",
        "        sns.set(font_scale=.8)\r\n",
        "        g = sns.heatmap(\r\n",
        "            products,\r\n",
        "            xticklabels=[textwrap.fill(e,15) for e in self.random],\r\n",
        "            yticklabels=[textwrap.fill(e,40) for e in self.random],\r\n",
        "            vmin=0,\r\n",
        "            vmax=1,\r\n",
        "            cmap=\"YlOrRd\",\r\n",
        "            mask=mask)\r\n",
        "        g.figure.set_size_inches(10,8)\r\n",
        "        plt.xticks(rotation=0)\r\n",
        "        g.set_title(\"Semantic Textual Similarity\")\r\n",
        "        plt.show()\r\n",
        "\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-208b091c3946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Research_Methods/Course_Project/YouTube_Data/Unicode_Files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;31m#print(os.listdir())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'youTubeVideosUTF.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Research_Methods/Course_Project/YouTube_Data/Unicode_Files'"
          ]
        }
      ]
    }
  ]
}