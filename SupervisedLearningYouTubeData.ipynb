{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasDeepLearning1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/thomasawolff/Machine-Learning-Projects/blob/master/KerasDeepLearning1.ipynb",
      "authorship_tag": "ABX9TyMsp3mFW95NvW3yU5rsskcv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomasawolff/YouTube-Video-Comments-Project/blob/main/SupervisedLearningYouTubeData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgyhlzRO6oso",
        "outputId": "5bfe0faf-b622-4621-d8f7-40749a416ddb"
      },
      "source": [
        "import nltk\r\n",
        "import os\r\n",
        "import random\r\n",
        "import pickle\r\n",
        "import textwrap\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from textblob import TextBlob\r\n",
        "from google.colab import drive\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "nltk.download(\"stopwords\")\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from datetime import datetime\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.metrics import classification_report,confusion_matrix\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "\r\n",
        "print(os.chdir('/content/drive/MyDrive/Research_Methods/Course_Project/YouTube_Data/Unicode_Files'))\r\n",
        "#print(os.listdir())\r\n",
        "url = 'youTubeVideosUTF.csv'\r\n",
        "\r\n",
        "\r\n",
        "class textAnalytics(object):\r\n",
        "\r\n",
        "    def __init__(self,file1,\r\n",
        "                 numClusters=3,\r\n",
        "                 dataFeature1=None,\r\n",
        "                 dataFeature2=None,\r\n",
        "                 dataFeature3=None,\r\n",
        "                 dataFeature4=None,\r\n",
        "                 ):\r\n",
        "        self.number_clusters = numClusters\r\n",
        "        self.dataFeature1 = dataFeature1\r\n",
        "        self.dataFeature2 = dataFeature2\r\n",
        "        self.dataFeature3 = dataFeature3\r\n",
        "        self.dataFeature4 = dataFeature4\r\n",
        "        data_df = pd.read_csv(file1,low_memory=False)\r\n",
        "        self.token_pattern = '(?u)\\\\b\\\\w+\\\\b'\r\n",
        "        review_df_All = data_df[[self.dataFeature1,self.dataFeature2,self.dataFeature3,self.dataFeature4]]\r\n",
        "        videoTitles = pd.read_csv('YouTubeVideoTitles.csv')\r\n",
        "        self.dataComm = pd.merge(videoTitles, review_df_All, on = dataFeature1)\r\n",
        "        self.stopWords = stopwords.words('english')\r\n",
        "                \r\n",
        "\r\n",
        "    def sentimentAnalysis(self):\r\n",
        "        pol = []\r\n",
        "        sub = []\r\n",
        "        for i in self.dataComm.commentText.values:\r\n",
        "            try:\r\n",
        "                analysis = TextBlob(i)\r\n",
        "                pol.append(round(analysis.sentiment.polarity,2))\r\n",
        "            except:\r\n",
        "                pol.append(0)\r\n",
        "\r\n",
        "        for i in self.dataComm.commentText.values:\r\n",
        "            try:\r\n",
        "                analysis = TextBlob(i)\r\n",
        "                sub.append(round(analysis.sentiment.subjectivity,2))\r\n",
        "            except:\r\n",
        "                sub.append(0)\r\n",
        "        self.dataComm['polarity']=pol\r\n",
        "        self.dataComm['subjectivity']=sub\r\n",
        "        #print(self.dataComm['polarity'])\r\n",
        "        self.dataComm.loc[self.dataComm['polarity'] < 0, 'sentimentBucket'] = -1\r\n",
        "        self.dataComm.loc[self.dataComm['polarity'] == 0, 'sentimentBucket'] = 0\r\n",
        "        self.dataComm.loc[self.dataComm['polarity'] > 0, 'sentimentBucket'] = 1\r\n",
        "        #dataComm .to_csv('youTubeVideosSentimentAnalysisSample10000.csv',sep=',',encoding='utf-8')\r\n",
        "        ##                    videoID       categoryID  views  ...    replies  polarity   subjectivity\r\n",
        "        ##          251449  LLGENw4C1jk          17   1002386  ...      0.0      0.50          0.50\r\n",
        "        ##          39834   3VVnY86ulA8          22    802134  ...      0.0      0.00          0.10\r\n",
        "        ##          203460  iA86imHKCMw          17   3005399  ...      0.0     -0.08          0.69\r\n",
        "        ##          345225  RRkdV_xmYOI          23    367544  ...      0.0      0.13          0.76\r\n",
        "        ##          402953  vQ3XgMKAgxc          10  51204658  ...      0.0      0.25          0.50\r\n",
        "        \r\n",
        "\r\n",
        "        \r\n",
        "    def dataModify(self):\r\n",
        "        self.sentimentAnalysis()\r\n",
        "        self.dataComm  = self.dataComm[[self.dataFeature1,self.dataFeature2,self.dataFeature3,self.dataFeature4,\\\r\n",
        "                                        'polarity','subjectivity','sentimentBucket']]\r\n",
        "        # using the polarity and subjectivity data for k means\r\n",
        "        # accessing them by their index but calling them by column name\r\n",
        "        # iloc[:,[self.dataComm.columns.get_loc('polarity'): iloc calls for the index location of the\r\n",
        "        # column polarity which is called by column name through get_loc which returns the index position of that column\r\n",
        "        self.X = self.dataComm.iloc[:,[self.dataComm.columns.get_loc('polarity'),self.dataComm.columns.get_loc('subjectivity')]].values\r\n",
        "\r\n",
        "\r\n",
        "      \r\n",
        "    def kMeansClustering(self):\r\n",
        "        self.dataModify()\r\n",
        "        kmeans = KMeans(self.number_clusters, init = 'k-means++',max_iter=300,n_init=10)\r\n",
        "        self.dataComm['clusters'] = kmeans.fit_predict(self.X)\r\n",
        "        return self.dataComm\r\n",
        "\r\n",
        "\r\n",
        "go = textAnalytics(url,\r\n",
        "                   numClusters = 5,\r\n",
        "                   dataFeature1 = 'videoID', \r\n",
        "                   dataFeature2 = 'categoryID',\r\n",
        "                   dataFeature3 = 'views',\r\n",
        "                   dataFeature4 = 'commentText')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def pandasAggregate():\r\n",
        "    data = go.kMeansClustering()\r\n",
        "    \r\n",
        "    dataPolarity = data[['videoID','sentimentBucket']].copy()\r\n",
        "    dataSubjectivity = data[['videoID','subjectivity']].copy()\r\n",
        "    dataClusters = data[['videoID','clusters']].copy()\r\n",
        "\r\n",
        "    # this code partitions the data by video ID and counts the number of values in the sentiment bucket column\r\n",
        "    # giving each row an incremented value which is then used for the pivot of the data\r\n",
        "    dataPolarity['dataRowNumSentiment'] = dataPolarity.sort_values(['videoID','sentimentBucket'], ascending=[True,False])\\\r\n",
        "             .groupby(['videoID'])\\\r\n",
        "             .cumcount() + 1\r\n",
        "\r\n",
        "    # this code partitions the data by video ID and counts the number of values in the subjectivity column\r\n",
        "    # giving each row an incremented value which is then used for the pivot of the data\r\n",
        "    dataSubjectivity['dataRowNumSubjectivity'] = dataSubjectivity.sort_values(['videoID','subjectivity'], ascending=[True,False])\\\r\n",
        "             .groupby(['videoID'])\\\r\n",
        "             .cumcount() + 1\r\n",
        "\r\n",
        "    # this code partitions the data by video ID and counts the number of values in the clusters column\r\n",
        "    # giving each row an incremented value which is then used for the pivot of the data\r\n",
        "    dataClusters['dataRowNumClusters'] = dataClusters.sort_values(['videoID','clusters'], ascending=[True,False])\\\r\n",
        "             .groupby(['videoID'])\\\r\n",
        "             .cumcount() + 1\r\n",
        "\r\n",
        "    # this code pivots the data using the fields created above. All values in these fields with be on one row per video ID\r\n",
        "    sentimentPivot = dataPolarity.pivot(index='videoID', columns='dataRowNumSentiment', values='sentimentBucket')\r\n",
        "    subjectivityPivot = dataSubjectivity.pivot(index='videoID', columns='dataRowNumSubjectivity', values='subjectivity')\r\n",
        "    clustersPivot = dataClusters.pivot(index='videoID', columns='dataRowNumClusters', values='clusters')\r\n",
        "\r\n",
        "\r\n",
        "    # I split up this part of the project into csv files since running all this data through the\r\n",
        "    # pipeline would take a long time especially when I was developing the predictive model.\r\n",
        "    # This takes the columns of index 20 through 21 for each data set\r\n",
        "    sentimentPivot = sentimentPivot.iloc[:,[20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,39,40]]\r\n",
        "    sentimentPivot.to_csv('SentimentPartition.csv')\r\n",
        "    subjectivityPivot = subjectivityPivot.iloc[:,[20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,39,40]]\r\n",
        "    subjectivityPivot.to_csv('SubjectivityPartition.csv')\r\n",
        "    clustersPivot = clustersPivot.iloc[:,[20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,39,40]]\r\n",
        "    clustersPivot.to_csv('ClustersPartition.csv')\r\n",
        "    return data\r\n",
        "\r\n",
        "#pandasAggregate()\r\n",
        "\r\n",
        "\r\n",
        "def dataMerge():\r\n",
        "    #df = pandasAggregate()\r\n",
        "    #np.seterr(divide = 'ignore')\r\n",
        "    df = pd.read_csv('youTubeVideosUTF.csv',low_memory=False)\r\n",
        "    df = df[['videoID','views','categoryID']].drop_duplicates()\r\n",
        "    df = df.set_index('videoID')\r\n",
        "   \r\n",
        "    clusters = pd.read_csv('ClustersPartition.csv')\r\n",
        "    clusters['41'].replace('', np.nan, inplace=True) # replacing empty cells in row 41 with nan's\r\n",
        "    clusters.dropna(subset=['41'], inplace=True) # dropping rows with nan's to create continuous rows of data\r\n",
        "    clusters = clusters.set_index('videoID')\r\n",
        "    \r\n",
        "    subject = pd.read_csv('SubjectivityPartition.csv')\r\n",
        "    subject['41'].replace('', np.nan, inplace=True)\r\n",
        "    subject.dropna(subset=['41'], inplace=True)\r\n",
        "    subject = subject.set_index('videoID')\r\n",
        "    \r\n",
        "    sentiment = pd.read_csv('SentimentPartition.csv')\r\n",
        "    sentiment['41'].replace('', np.nan, inplace=True)\r\n",
        "    sentiment.dropna(subset=['41'], inplace=True)\r\n",
        "    sentiment = sentiment.set_index('videoID')\r\n",
        "\r\n",
        "    merge2 = pd.merge(df, clusters, on = 'videoID') # merging together the datasets by videoID\r\n",
        "    merge3 = pd.merge(merge2, subject, on = 'videoID')\r\n",
        "    merge4 = pd.merge(merge3, sentiment, on = 'videoID')\r\n",
        "\r\n",
        "    # doing log transform of the views field\r\n",
        "    merge4['views'] = np.log2(merge4['views'])\r\n",
        "\r\n",
        "    # creating value buckets for the views field which will become a target variable for the model\r\n",
        "    merge4.loc[merge4['views'] < 20, 'viewsBucket'] = '1'\r\n",
        "    #merge4.loc[(merge4['views'] > 18) & (merge4['views'] <= 20), 'viewsBucket'] = '2'\r\n",
        "    #merge4.loc[(merge4['views'] > 20) & (merge4['views'] <= 22), 'viewsBucket'] = '3'\r\n",
        "    merge4.loc[merge4['views'] > 20, 'viewsBucket'] = '2'\r\n",
        "\r\n",
        "    #print(round(merge4['views'].describe(include='all')),2)\r\n",
        "    ##    25%        18.0\r\n",
        "    ##    50%        20.0\r\n",
        "    ##    75%        22.0\r\n",
        "    ##    max        29.0\r\n",
        "    ##    Name: views, dtype: float64 2\r\n",
        "    \r\n",
        "    del merge4['views']\r\n",
        "    merge4.to_csv('dataCombined.csv')\r\n",
        "    \r\n",
        "    return merge4\r\n",
        "\r\n",
        "    ##    videoID        categoryID views  ...   SentimentKBucket40  viewsBucket                                \r\n",
        "    ##    _0d3XbH12cs          10   18.0  ...                   1       2\r\n",
        "    ##    _38JDGnr0vA          15   24.0  ...                   1       4\r\n",
        "    ##    _4PLKxYZUPc          22   21.0  ...                   1       3\r\n",
        "    ##    _5wCA9OM00o          22   19.0  ...                   1       2\r\n",
        "    ##    _5ZrSKpbdSg          28   19.0  ...                   1       2\r\n",
        "    ##    ...                 ...    ...  ...                 ...          ...\r\n",
        "    ##    zyPIdeF4NFI          22   19.0  ...                   1       2\r\n",
        "    ##    ZYQ1cVRtMZU          26   24.0  ...                   1       4\r\n",
        "    ##    ZYSjPZUqLdk          22   21.0  ...                   1       3\r\n",
        "    ##    zZ2CLmvqfXg          24   22.0  ...                   1       3\r\n",
        "    ##    Z-zdIGxOJ4M          10   19.0  ...                   1       2\r\n",
        "    ##\r\n",
        "    ##    [2661 rows x 69 columns]\r\n",
        "\r\n",
        "#print(dataMerge())\r\n",
        "\r\n",
        "def modelPredictionsLR(operation):\r\n",
        "    data = dataMerge()\r\n",
        "    \r\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, data['viewsBucket'], test_size=0.2, random_state=1)\r\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\r\n",
        "\r\n",
        "    # deleting the viewsBucket field from the X train, validate, and test sets\r\n",
        "    del X_train['viewsBucket']\r\n",
        "    del X_test['viewsBucket']\r\n",
        "    del X_val['viewsBucket']\r\n",
        "\r\n",
        "    #try: \r\n",
        "    modelPCA = PCA(n_components = 2).fit(X_train)\r\n",
        "    print('Variance Explained by PCA model:',modelPCA.explained_variance_ratio_)\r\n",
        "    print('Singular values of PCA model:',modelPCA.singular_values_)\r\n",
        "    modelLR = LogisticRegression()\r\n",
        "    #except ValueError: pass\r\n",
        "\r\n",
        "    # performing principle components analysis to reduce the number of fields\r\n",
        "    # and use the eigenvalues as the data for modeling\r\n",
        "    X_train_PCA = modelPCA.transform(X_train)\r\n",
        "\r\n",
        "    # performing Logistic regression on the new PCA model\r\n",
        "    modelLR.fit(X_train_PCA,y_train)\r\n",
        "\r\n",
        "    #print('Train Performance Logistic Regression with PCA: '+str(round(modelLR.score(X_train_PCA,y_train),2)))\r\n",
        "    #predictions = modelLR.predict(X_train_PCA)\r\n",
        "    #print(confusion_matrix(y_train,predictions))\r\n",
        "\r\n",
        "    if operation == 'cross validation':\r\n",
        "        print('Cross Validation scores from 8 iterations:')\r\n",
        "        scores = cross_val_score(modelLR, X_train_PCA, y_train, cv=8)\r\n",
        "        print(scores)\r\n",
        "    \r\n",
        "    elif operation == 'validation set':\r\n",
        "        X_val_PCA = modelPCA.transform(X_val)\r\n",
        "        predictions = modelLR.predict(X_val_PCA)\r\n",
        "        print('Validation Performance Logistic Regression with PCA: '+str(round(+modelLR.score(X_val_PCA,y_val),2)))\r\n",
        "        print('Confusion Matrix:')\r\n",
        "        print(confusion_matrix(y_val,predictions))\r\n",
        "        print('Classification Report:')\r\n",
        "        print(classification_report(y_val,predictions))\r\n",
        "        \r\n",
        "    elif operation == 'test set':\r\n",
        "        X_test_PCA = modelPCA.transform(X_test)\r\n",
        "        predictions = modelLR.predict(X_test_PCA)\r\n",
        "        print('Test Performance Logistic Regression with PCA: '+str(round(+modelLR.score(X_test_PCA,y_test),2)))\r\n",
        "        print('Confusion Matrix:')\r\n",
        "        print(confusion_matrix(y_test,predictions))\r\n",
        "        print('Classification Report:')\r\n",
        "        print(classification_report(y_test,predictions))\r\n",
        "\r\n",
        "\r\n",
        "    with open('YouTubeModelPickle','wb') as p: # saving the trained model into a pickle file\r\n",
        "        pickle.dump(modelLR,p)\r\n",
        "    \r\n",
        "    input_ = input('Hit Enter to leave')\r\n",
        "\r\n",
        "    ##    Variance Explained by PCA model: [0.84019443 0.11170318]\r\n",
        "    ##    Singlular values of PCA model: [286.4529428  104.44704467]\r\n",
        "    ##    Train Performance Logistic Regression with PCA: 0.73\r\n",
        "    ##    [[542 196]\r\n",
        "    ##     [235 622]]\r\n",
        "    ##    Test Performance Logistic Regression with PCA: 0.71\r\n",
        "    ##    Confusion Matrix:\r\n",
        "    ##    [[184  65]\r\n",
        "    ##     [ 90 193]]\r\n",
        "    ##    Classification Report:\r\n",
        "    ##                  precision    recall  f1-score   support\r\n",
        "    ##\r\n",
        "    ##               1       0.67      0.74      0.70       249\r\n",
        "    ##               2       0.75      0.68      0.71       283\r\n",
        "    ##\r\n",
        "    ##        accuracy                           0.71       532\r\n",
        "    ##       macro avg       0.71      0.71      0.71       532\r\n",
        "    ##    weighted avg       0.71      0.71      0.71       532\r\n",
        "\r\n",
        "\r\n",
        "modelPredictionsLR('cross validation')\r\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "None\n",
            "Variance Explained by PCA model: [0.61745155 0.32753289]\n",
            "Singular values of PCA model: [287.46980561 209.3718304 ]\n",
            "Cross Validation scores from 8 iterations:\n",
            "[0.695      0.73       0.77       0.67839196 0.73366834 0.72361809\n",
            " 0.69849246 0.69346734]\n",
            "Hit Enter to leave\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}